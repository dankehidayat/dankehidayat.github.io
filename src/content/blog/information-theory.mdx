---
title: "Information Theory and Entropy"
date: 2024-03-01
author: "Danke Hidayat"
excerpt: "Quantifying information and uncertainty in communication systems."
categories: ["Mathematics", "Computer Science"]
tags: ["Information Theory", "Entropy", "Data Compression"]
labels: ["Theoretical Foundation", "Practical Applications"]
---

# Information Theory and Entropy

## Shannon Entropy

$$
H(X) = -\sum_{i=1}^{n} P(x_i) \log_2 P(x_i)
$$

## Mutual Information

$$
I(X;Y) = \sum_{x,y} P(x,y) \log \frac{P(x,y)}{P(x)P(y)}
$$
